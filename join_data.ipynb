{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join the data downloaded from Entso-e to create a unified DataFrame\n",
    "\n",
    "Create one datframe for day-ahead and one for week-ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from Name_convention_dictionaries import PsrTypeDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the data type of the dates is datetime:\n",
    "dateparse = lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename[:-38]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join all the files for day-ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all the day-ahead files\n",
    "folder_name = 'data_day_ahead'\n",
    "files_in_dir = os.listdir(\"./\"+folder_name+\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal is to create a DataFrame for each document and process type. This means, all the data from regarding e.g. total load is saved in one big DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_if_loaded = []\n",
    "list_of_dfs = []\n",
    "\n",
    "# take the first file in the folder\n",
    "for i in files_in_dir:\n",
    "    group_files = []\n",
    "    \n",
    "    # check if the file has been used before\n",
    "    if i == '.DS_Store':\n",
    "        pass\n",
    "    \n",
    "    elif i[:-38] not in check_if_loaded:\n",
    "        \n",
    "        # you want to group all files in the folder that belong to the same document and process type\n",
    "        # search for all files that have the same characters at the beginning of the file name\n",
    "        for j in files_in_dir:\n",
    "            \n",
    "            # if the selected file has the same characters at the beginning of the file name, save the\n",
    "            # path in the group_files list\n",
    "            if i[:-38] == j[:-38]:\n",
    "                group_files.append(\"./\"+folder_name+\"/\"+j)\n",
    "        \n",
    "        # save the first file in the group_files list as a dataframe in memory\n",
    "        df = pd.read_csv(group_files[0],parse_dates=['Date'], date_parser=dateparse)\n",
    "        \n",
    "        # iterate through all the remaining files in the group_files list and store them as a dataframe\n",
    "        # in memory. Then concatenate all dataframes\n",
    "        for k in group_files[1:]:\n",
    "            df2 = pd.read_csv(k,parse_dates=['Date'], date_parser=dateparse)\n",
    "            df = pd.concat([df,df2])\n",
    "\n",
    "        # Finally, sort the values in the dataframe by datetime and append the dataframe to the\n",
    "        # list_of_dfs. This will be used later to merge all dataframes together\n",
    "        df = df.sort_values(by=[\"Date\"])\n",
    "        list_of_dfs.append(df)\n",
    "        \n",
    "        # Append the first file to the check_if_loaded list to make sure all files with the same document\n",
    "        # and process type are ignored for the next iteration in the loop\n",
    "        check_if_loaded.append(i[:-38])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# search for duplicates and get rid of them\n",
    "for i in list_of_dfs:\n",
    "    print(i.duplicated().sum())\n",
    "    i.drop_duplicates(inplace=True)\n",
    "    print(i.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = reduce(lambda  left,right: pd.merge(left,right,on=['Date'],\n",
    "                                            how='outer'), list_of_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged.sort_values(by=[\"Date\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the new dataframe as a csv file\n",
    "df_merged.to_csv(\"Day_ahead_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all columns have been stored\n",
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many nan values are in the DataFrame\n",
    "df_merged.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join all the files for week-ahead predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all the day-ahead files\n",
    "folder_name = 'data_week_ahead'\n",
    "files_in_dir = os.listdir(\"./\"+folder_name+\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task for the week-ahead prediction is much simpler than day-ahead (as there are no actuals or generation values). use a simpler implementation to save some memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of the paths to the files. So they can be saved as DataFrames\n",
    "list_of_paths = []\n",
    "for name in files_in_dir:\n",
    "    list_of_paths.append(\"./\"+folder_name+\"/\"+name)\n",
    "\n",
    "# store the DataFrames in memory and concatenate them\n",
    "df_week = pd.read_csv(list_of_paths[0], parse_dates=['min_date', 'max_date'], date_parser=dateparse)\n",
    "for file in list_of_paths[1:]:\n",
    "    df2 = pd.read_csv(file, parse_dates=['min_date', 'max_date'], date_parser=dateparse)\n",
    "    df_week = pd.concat([df_week, df2])\n",
    "    \n",
    "df_week.sort_values(by=['min_date', 'max_date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of all duplicates before saving the data as a csv\n",
    "df_week.drop_duplicates(inplace=True)\n",
    "\n",
    "df_week.to_csv('Week_ahead_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_week.sample(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
